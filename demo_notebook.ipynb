{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sentiment Analysis API Demo\n",
        "\n",
        "This notebook demonstrates how to interact with the sentiment analysis API deployed in Docker.\n",
        "\n",
        "The API uses a DistilBERT model fine-tuned on the SST-2 dataset for sentiment analysis.\n",
        "\n",
        "**Prerequisites:**\n",
        "- Docker and docker-compose installed\n",
        "- API running (start with: `docker-compose up -d`)\n",
        "- API accessible at `http://localhost` (or your server address)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing requests...\n",
            "‚úÖ requests installed successfully\n",
            "Installing pandas...\n",
            "‚úÖ pandas installed successfully\n",
            "‚úÖ All packages loaded successfully!\n",
            "\n",
            "API Base URL: http://localhost\n",
            "Predict Endpoint: http://localhost/predict\n",
            "Health Endpoint: http://localhost/health\n"
          ]
        }
      ],
      "source": [
        "# Install required packages if not already installed\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "def install_if_missing(package):\n",
        "    try:\n",
        "        __import__(package)\n",
        "    except ImportError:\n",
        "        print(f\"Installing {package}...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"-q\"])\n",
        "        print(f\"‚úÖ {package} installed successfully\")\n",
        "\n",
        "# Install packages\n",
        "install_if_missing(\"requests\")\n",
        "install_if_missing(\"pandas\")\n",
        "\n",
        "# Now import everything\n",
        "import requests\n",
        "import json\n",
        "import time\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "\n",
        "# API endpoint (adjust if running on different host/port)\n",
        "API_BASE_URL = \"http://localhost\"\n",
        "PREDICT_ENDPOINT = f\"{API_BASE_URL}/predict\"\n",
        "HEALTH_ENDPOINT = f\"{API_BASE_URL}/health\"\n",
        "\n",
        "print(f\"‚úÖ All packages loaded successfully!\")\n",
        "print(f\"\\nAPI Base URL: {API_BASE_URL}\")\n",
        "print(f\"Predict Endpoint: {PREDICT_ENDPOINT}\")\n",
        "print(f\"Health Endpoint: {HEALTH_ENDPOINT}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Health Check\n",
        "\n",
        "First, let's verify the API is running and healthy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Status Code: 200\n",
            "Response: {\n",
            "  \"device\": \"cpu\",\n",
            "  \"model_loaded\": true,\n",
            "  \"status\": \"healthy\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Check API health\n",
        "try:\n",
        "    response = requests.get(HEALTH_ENDPOINT, timeout=5)\n",
        "    print(f\"Status Code: {response.status_code}\")\n",
        "    print(f\"Response: {json.dumps(response.json(), indent=2)}\")\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error connecting to API: {e}\")\n",
        "    print(\"Make sure the API is running with: docker-compose up -d\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Single Request Example\n",
        "\n",
        "Let's make a simple prediction request.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input text: I absolutely love this product! It's amazing and works perfectly.\n",
            "\n",
            "Status Code: 200\n",
            "Response:\n",
            "{\n",
            "  \"negative_score\": 0.0001,\n",
            "  \"positive_score\": 0.9999,\n",
            "  \"score\": 0.9999,\n",
            "  \"sentiment\": \"POSITIVE\",\n",
            "  \"text\": \"I absolutely love this product! It's amazing and works perfectly.\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Example 1: Positive sentiment\n",
        "text1 = \"I absolutely love this product! It's amazing and works perfectly.\"\n",
        "payload = {\"text\": text1}\n",
        "\n",
        "print(f\"Input text: {text1}\\n\")\n",
        "response = requests.post(PREDICT_ENDPOINT, json=payload)\n",
        "result = response.json()\n",
        "\n",
        "print(f\"Status Code: {response.status_code}\")\n",
        "print(f\"Response:\\n{json.dumps(result, indent=2)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 2: Negative sentiment\n",
        "text2 = \"This is terrible. I'm very disappointed with the quality and service.\"\n",
        "payload = {\"text\": text2}\n",
        "\n",
        "print(f\"Input text: {text2}\\n\")\n",
        "response = requests.post(PREDICT_ENDPOINT, json=payload)\n",
        "result = response.json()\n",
        "\n",
        "print(f\"Status Code: {response.status_code}\")\n",
        "print(f\"Response:\\n{json.dumps(result, indent=2)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Multiple Sequential Requests\n",
        "\n",
        "Let's test multiple requests sequentially to see the API's response time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sequential Requests:\n",
            "\n",
            "1. POSITIVE (0.9999) - 130.99ms\n",
            "2. NEGATIVE (0.9862) - 437.75ms\n",
            "3. NEGATIVE (0.9968) - 110.52ms\n",
            "4. POSITIVE (0.9999) - 134.67ms\n",
            "5. NEGATIVE (0.9998) - 122.51ms\n",
            "6. POSITIVE (0.9999) - 113.55ms\n",
            "7. NEGATIVE (0.9998) - 125.60ms\n",
            "8. POSITIVE (0.9998) - 117.03ms\n",
            "\n",
            "Total time: 1.29s\n",
            "Average time per request: 161.77ms\n"
          ]
        }
      ],
      "source": [
        "# Test texts with various sentiments\n",
        "test_texts = [\n",
        "    \"This movie is fantastic! I highly recommend it.\",\n",
        "    \"The service was okay, nothing special.\",\n",
        "    \"I hate waiting in long lines. Very frustrating experience.\",\n",
        "    \"The food was delicious and the atmosphere was perfect.\",\n",
        "    \"Not impressed at all. Poor quality and overpriced.\",\n",
        "    \"Amazing customer service! They went above and beyond.\",\n",
        "    \"The product broke after one day. Complete waste of money.\",\n",
        "    \"Great value for money. Will definitely buy again!\"\n",
        "]\n",
        "\n",
        "print(\"Sequential Requests:\\n\")\n",
        "start_time = time.time()\n",
        "results = []\n",
        "\n",
        "for i, text in enumerate(test_texts, 1):\n",
        "    payload = {\"text\": text}\n",
        "    request_start = time.time()\n",
        "    response = requests.post(PREDICT_ENDPOINT, json=payload)\n",
        "    request_time = time.time() - request_start\n",
        "    \n",
        "    result = response.json()\n",
        "    results.append({\n",
        "        \"text\": text[:50] + \"...\" if len(text) > 50 else text,\n",
        "        \"sentiment\": result.get(\"sentiment\", \"N/A\"),\n",
        "        \"score\": result.get(\"score\", 0),\n",
        "        \"response_time_ms\": round(request_time * 1000, 2)\n",
        "    })\n",
        "    print(f\"{i}. {result.get('sentiment', 'N/A')} ({result.get('score', 0):.4f}) - {request_time*1000:.2f}ms\")\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print(f\"\\nTotal time: {total_time:.2f}s\")\n",
        "print(f\"Average time per request: {total_time/len(test_texts)*1000:.2f}ms\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Parallel Requests Demonstration\n",
        "\n",
        "Now let's demonstrate the API's ability to handle multiple parallel requests simultaneously. This showcases the NGINX + Gunicorn architecture's capability to process concurrent requests.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Making 16 parallel requests...\n",
            "\n",
            "Parallel Request Results:\n",
            "\n",
            "‚úì  1. POSITIVE (0.9999) - 549.77ms - I'm so happy with my purchase!\n",
            "‚úì  2. POSITIVE (0.9999) - 541.65ms - Outstanding service and great product qu...\n",
            "‚úì  3. NEGATIVE (0.9998) - 543.77ms - This is the worst experience ever.\n",
            "‚úì  4. NEGATIVE (0.9997) - 559.96ms - Terrible customer support. Will not reco...\n",
            "‚úì  5. POSITIVE (0.9999) - 995.01ms - Great product, fast shipping, excellent ...\n",
            "‚úì  6. NEGATIVE (0.9997) - 1002.01ms - Horrible experience from start to finish...\n",
            "‚úì  7. NEGATIVE (0.9998) - 1001.54ms - Very poor quality. Do not buy.\n",
            "‚úì  8. NEGATIVE (0.9998) - 1061.86ms - Not worth the price. Very disappointed.\n",
            "‚úì  9. POSITIVE (0.9999) - 1056.46ms - Fantastic value! Highly satisfied.\n",
            "‚úì 10. POSITIVE (0.9999) - 1278.98ms - Perfect! Exceeded my expectations.\n",
            "‚úì 11. POSITIVE (0.9998) - 1304.66ms - The quality is excellent and delivery wa...\n",
            "‚úì 12. POSITIVE (0.9999) - 1305.26ms - Love it! Exactly what I was looking for.\n",
            "‚úì 13. NEGATIVE (0.9991) - 1301.08ms - Awful product. Returned immediately.\n",
            "‚úì 14. NEGATIVE (0.9998) - 1439.38ms - Waste of money. Complete garbage.\n",
            "‚úì 15. POSITIVE (0.9999) - 1480.45ms - Amazing features and easy to use.\n",
            "‚úì 16. NEGATIVE (0.9995) - 1498.71ms - Poor quality materials. Broke immediatel...\n",
            "\n",
            "======================================================================\n",
            "Total time for 16 parallel requests: 1.51s\n",
            "Average time per request: 94.49ms\n",
            "Throughput: 10.58 requests/second\n",
            "Successful requests: 16/16\n"
          ]
        }
      ],
      "source": [
        "def make_prediction(text):\n",
        "    \"\"\"Helper function to make a prediction request.\"\"\"\n",
        "    payload = {\"text\": text}\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        response = requests.post(PREDICT_ENDPOINT, json=payload, timeout=30)\n",
        "        elapsed = time.time() - start_time\n",
        "        result = response.json()\n",
        "        return {\n",
        "            \"text\": text[:40] + \"...\" if len(text) > 40 else text,\n",
        "            \"sentiment\": result.get(\"sentiment\", \"ERROR\"),\n",
        "            \"score\": result.get(\"score\", 0),\n",
        "            \"response_time_ms\": round(elapsed * 1000, 2),\n",
        "            \"status_code\": response.status_code,\n",
        "            \"success\": response.status_code == 200\n",
        "        }\n",
        "    except Exception as e:\n",
        "        elapsed = time.time() - start_time\n",
        "        return {\n",
        "            \"text\": text[:40] + \"...\" if len(text) > 40 else text,\n",
        "            \"sentiment\": \"ERROR\",\n",
        "            \"score\": 0,\n",
        "            \"response_time_ms\": round(elapsed * 1000, 2),\n",
        "            \"status_code\": 0,\n",
        "            \"success\": False,\n",
        "            \"error\": str(e)\n",
        "        }\n",
        "\n",
        "# Test with parallel requests\n",
        "parallel_texts = [\n",
        "    \"I'm so happy with my purchase!\",\n",
        "    \"This is the worst experience ever.\",\n",
        "    \"The quality is excellent and delivery was fast.\",\n",
        "    \"Not worth the price. Very disappointed.\",\n",
        "    \"Outstanding service and great product quality.\",\n",
        "    \"Terrible customer support. Will not recommend.\",\n",
        "    \"Love it! Exactly what I was looking for.\",\n",
        "    \"Poor quality materials. Broke immediately.\",\n",
        "    \"Fantastic value! Highly satisfied.\",\n",
        "    \"Waste of money. Complete garbage.\",\n",
        "    \"Amazing features and easy to use.\",\n",
        "    \"Horrible experience from start to finish.\",\n",
        "    \"Great product, fast shipping, excellent service.\",\n",
        "    \"Very poor quality. Do not buy.\",\n",
        "    \"Perfect! Exceeded my expectations.\",\n",
        "    \"Awful product. Returned immediately.\"\n",
        "]\n",
        "\n",
        "print(f\"Making {len(parallel_texts)} parallel requests...\\n\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Use ThreadPoolExecutor for parallel requests\n",
        "with ThreadPoolExecutor(max_workers=16) as executor:\n",
        "    futures = [executor.submit(make_prediction, text) for text in parallel_texts]\n",
        "    parallel_results = [future.result() for future in as_completed(futures)]\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "\n",
        "# Display results\n",
        "print(\"Parallel Request Results:\\n\")\n",
        "for i, result in enumerate(parallel_results, 1):\n",
        "    status = \"‚úì\" if result[\"success\"] else \"‚úó\"\n",
        "    print(f\"{status} {i:2d}. {result['sentiment']:8s} ({result['score']:.4f}) - {result['response_time_ms']:6.2f}ms - {result['text']}\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"Total time for {len(parallel_texts)} parallel requests: {total_time:.2f}s\")\n",
        "print(f\"Average time per request: {total_time/len(parallel_texts)*1000:.2f}ms\")\n",
        "print(f\"Throughput: {len(parallel_texts)/total_time:.2f} requests/second\")\n",
        "\n",
        "# Count successes\n",
        "successful = sum(1 for r in parallel_results if r[\"success\"])\n",
        "print(f\"Successful requests: {successful}/{len(parallel_texts)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Performance Comparison: Sequential vs Parallel\n",
        "\n",
        "Let's compare the performance difference between sequential and parallel request handling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sequential Requests:\n",
            "  Total time: 0.89s\n",
            "  Average: 111.41ms per request\n",
            "  Throughput: 8.98 req/s\n",
            "\n",
            "Parallel Requests:\n",
            "  Total time: 0.40s\n",
            "  Average: 50.42ms per request\n",
            "  Throughput: 19.83 req/s\n",
            "\n",
            "======================================================================\n",
            "Speedup: 2.21x faster with parallel requests\n",
            "Time saved: 0.49s (54.7% reduction)\n"
          ]
        }
      ],
      "source": [
        "# Performance comparison\n",
        "comparison_texts = parallel_texts[:8]  # Use first 8 texts for comparison\n",
        "\n",
        "# Sequential requests\n",
        "print(\"Sequential Requests:\")\n",
        "seq_start = time.time()\n",
        "seq_results = []\n",
        "for text in comparison_texts:\n",
        "    result = make_prediction(text)\n",
        "    seq_results.append(result)\n",
        "seq_time = time.time() - seq_start\n",
        "\n",
        "print(f\"  Total time: {seq_time:.2f}s\")\n",
        "print(f\"  Average: {seq_time/len(comparison_texts)*1000:.2f}ms per request\")\n",
        "print(f\"  Throughput: {len(comparison_texts)/seq_time:.2f} req/s\\n\")\n",
        "\n",
        "# Parallel requests\n",
        "print(\"Parallel Requests:\")\n",
        "par_start = time.time()\n",
        "with ThreadPoolExecutor(max_workers=8) as executor:\n",
        "    futures = [executor.submit(make_prediction, text) for text in comparison_texts]\n",
        "    par_results = [future.result() for future in as_completed(futures)]\n",
        "par_time = time.time() - par_start\n",
        "\n",
        "print(f\"  Total time: {par_time:.2f}s\")\n",
        "print(f\"  Average: {par_time/len(comparison_texts)*1000:.2f}ms per request\")\n",
        "print(f\"  Throughput: {len(comparison_texts)/par_time:.2f} req/s\\n\")\n",
        "\n",
        "# Comparison\n",
        "speedup = seq_time / par_time if par_time > 0 else 0\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"Speedup: {speedup:.2f}x faster with parallel requests\")\n",
        "print(f\"Time saved: {seq_time - par_time:.2f}s ({((seq_time - par_time)/seq_time*100):.1f}% reduction)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Results Summary and Visualization\n",
        "\n",
        "Let's create a summary DataFrame and visualize the results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results Summary:\n",
            "======================================================================\n",
            "\n",
            "Total Requests: 16\n",
            "Successful: 16\n",
            "Failed: 0\n",
            "\n",
            "Sentiment Distribution:\n",
            "sentiment\n",
            "POSITIVE    8\n",
            "NEGATIVE    8\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Response Time Statistics:\n",
            "  Mean: 1057.53ms\n",
            "  Median: 1059.16ms\n",
            "  Min: 541.65ms\n",
            "  Max: 1498.71ms\n",
            "  Std Dev: 346.00ms\n",
            "\n",
            "Confidence Score Statistics:\n",
            "  Mean: 0.9998\n",
            "  Median: 0.9998\n",
            "  Min: 0.9991\n",
            "  Max: 0.9999\n",
            "\n",
            "======================================================================\n",
            "Full Results Table:\n",
            "                                       text sentiment  score  response_time_ms\n",
            "             I'm so happy with my purchase!  POSITIVE 0.9999            549.77\n",
            "Outstanding service and great product qu...  POSITIVE 0.9999            541.65\n",
            "         This is the worst experience ever.  NEGATIVE 0.9998            543.77\n",
            "Terrible customer support. Will not reco...  NEGATIVE 0.9997            559.96\n",
            "Great product, fast shipping, excellent ...  POSITIVE 0.9999            995.01\n",
            "Horrible experience from start to finish...  NEGATIVE 0.9997           1002.01\n",
            "             Very poor quality. Do not buy.  NEGATIVE 0.9998           1001.54\n",
            "    Not worth the price. Very disappointed.  NEGATIVE 0.9998           1061.86\n",
            "         Fantastic value! Highly satisfied.  POSITIVE 0.9999           1056.46\n",
            "         Perfect! Exceeded my expectations.  POSITIVE 0.9999           1278.98\n",
            "The quality is excellent and delivery wa...  POSITIVE 0.9998           1304.66\n",
            "   Love it! Exactly what I was looking for.  POSITIVE 0.9999           1305.26\n",
            "       Awful product. Returned immediately.  NEGATIVE 0.9991           1301.08\n",
            "          Waste of money. Complete garbage.  NEGATIVE 0.9998           1439.38\n",
            "          Amazing features and easy to use.  POSITIVE 0.9999           1480.45\n",
            "Poor quality materials. Broke immediatel...  NEGATIVE 0.9995           1498.71\n"
          ]
        }
      ],
      "source": [
        "# Create DataFrame from parallel results\n",
        "df = pd.DataFrame(parallel_results)\n",
        "\n",
        "print(\"Results Summary:\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nTotal Requests: {len(df)}\")\n",
        "print(f\"Successful: {df['success'].sum()}\")\n",
        "print(f\"Failed: {(~df['success']).sum()}\")\n",
        "\n",
        "print(f\"\\nSentiment Distribution:\")\n",
        "print(df['sentiment'].value_counts())\n",
        "\n",
        "print(f\"\\nResponse Time Statistics:\")\n",
        "print(f\"  Mean: {df['response_time_ms'].mean():.2f}ms\")\n",
        "print(f\"  Median: {df['response_time_ms'].median():.2f}ms\")\n",
        "print(f\"  Min: {df['response_time_ms'].min():.2f}ms\")\n",
        "print(f\"  Max: {df['response_time_ms'].max():.2f}ms\")\n",
        "print(f\"  Std Dev: {df['response_time_ms'].std():.2f}ms\")\n",
        "\n",
        "print(f\"\\nConfidence Score Statistics:\")\n",
        "print(f\"  Mean: {df['score'].mean():.4f}\")\n",
        "print(f\"  Median: {df['score'].median():.4f}\")\n",
        "print(f\"  Min: {df['score'].min():.4f}\")\n",
        "print(f\"  Max: {df['score'].max():.4f}\")\n",
        "\n",
        "# Display full results table\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"Full Results Table:\")\n",
        "print(df[['text', 'sentiment', 'score', 'response_time_ms']].to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Error Handling Examples\n",
        "\n",
        "Let's test how the API handles edge cases and errors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing Error Handling:\n",
            "\n",
            "1. Empty text:\n",
            "   Status: 400\n",
            "   Response: {\n",
            "  \"error\": \"Text field is required and cannot be empty\"\n",
            "}\n",
            "\n",
            "2. Missing 'text' field:\n",
            "   Status: 400\n",
            "   Response: {\n",
            "  \"error\": \"No JSON data provided\"\n",
            "}\n",
            "\n",
            "3. Invalid JSON:\n",
            "   Status: 500\n",
            "4. Very long text (truncation test):\n",
            "   Status: 200\n",
            "   Sentiment: NEGATIVE\n",
            "   Score: 0.9974\n",
            "\n",
            "5. Text with special characters and emojis:\n",
            "   Status: 200\n",
            "   Sentiment: POSITIVE\n",
            "   Score: 0.9999\n"
          ]
        }
      ],
      "source": [
        "# Test error cases\n",
        "print(\"Testing Error Handling:\\n\")\n",
        "\n",
        "# Test 1: Empty text\n",
        "print(\"1. Empty text:\")\n",
        "response = requests.post(PREDICT_ENDPOINT, json={\"text\": \"\"})\n",
        "print(f\"   Status: {response.status_code}\")\n",
        "print(f\"   Response: {json.dumps(response.json(), indent=2)}\\n\")\n",
        "\n",
        "# Test 2: Missing text field\n",
        "print(\"2. Missing 'text' field:\")\n",
        "response = requests.post(PREDICT_ENDPOINT, json={})\n",
        "print(f\"   Status: {response.status_code}\")\n",
        "print(f\"   Response: {json.dumps(response.json(), indent=2)}\\n\")\n",
        "\n",
        "# Test 3: Invalid JSON\n",
        "print(\"3. Invalid JSON:\")\n",
        "try:\n",
        "    response = requests.post(PREDICT_ENDPOINT, data=\"not json\")\n",
        "    print(f\"   Status: {response.status_code}\")\n",
        "except Exception as e:\n",
        "    print(f\"   Error: {e}\\n\")\n",
        "\n",
        "# Test 4: Very long text (should be truncated)\n",
        "print(\"4. Very long text (truncation test):\")\n",
        "long_text = \"This is a test. \" * 1000\n",
        "response = requests.post(PREDICT_ENDPOINT, json={\"text\": long_text})\n",
        "print(f\"   Status: {response.status_code}\")\n",
        "result = response.json()\n",
        "print(f\"   Sentiment: {result.get('sentiment')}\")\n",
        "print(f\"   Score: {result.get('score')}\\n\")\n",
        "\n",
        "# Test 5: Special characters and emojis\n",
        "print(\"5. Text with special characters and emojis:\")\n",
        "special_text = \"I love this! üòç It's amazing!!! üéâüéä\"\n",
        "response = requests.post(PREDICT_ENDPOINT, json={\"text\": special_text})\n",
        "print(f\"   Status: {response.status_code}\")\n",
        "result = response.json()\n",
        "print(f\"   Sentiment: {result.get('sentiment')}\")\n",
        "print(f\"   Score: {result.get('score')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. API Architecture Demonstration\n",
        "\n",
        "This API demonstrates a production-ready architecture with:\n",
        "\n",
        "- **NGINX**: Reverse proxy and load balancer (handles incoming connections)\n",
        "- **Gunicorn**: WSGI server with multiple workers (processes Python requests)\n",
        "- **Flask**: Web framework (handles application logic)\n",
        "- **DistilBERT**: ML model for sentiment analysis\n",
        "\n",
        "The parallel request handling showcases how NGINX distributes requests across Gunicorn workers, allowing multiple predictions to be processed simultaneously.\n",
        "\n",
        "### Key Features Demonstrated:\n",
        "\n",
        "1. ‚úÖ Health check endpoint for monitoring\n",
        "2. ‚úÖ JSON-based API with proper error handling\n",
        "3. ‚úÖ Support for parallel/concurrent requests\n",
        "4. ‚úÖ Fast inference with optimized model\n",
        "5. ‚úÖ Production-ready architecture with proper layering\n",
        "\n",
        "---\n",
        "\n",
        "**Note:** Make sure the Docker containers are running before executing this notebook:\n",
        "```bash\n",
        "docker-compose up -d\n",
        "```\n",
        "\n",
        "To stop the containers:\n",
        "```bash\n",
        "docker-compose down\n",
        "```\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
